# Lab 1 -  Report

- Student ID: 518021911058
- Student Name: 沈瑜石
- Date: 2021/5/9

## Framework analysis
### Master Framework
* The master node (in the lab is the goroutine) calls **Distributed** function to run.
* **Distributed** calls **startRPCServer** to start master's RPC server in a listener goroutine. The master listener goroutine runs a loop to accept request from workers, and onces accept, it handle the request in another server routine so that not block the listener.
* Then **Distributed** runs a goroutine by calling **run**, which runs the main logic of master. The **Distributed** then returns, so a **Wait** function is needed, which waits a 'done' by channel.
* The **run** call **schedule** twice to allocate map job and reduce job sequentially. It gives **schedule** a channel which is filled by goroutine **forwardRegistration**. The latter repeatedly locks the master structure and reads member *workers*. It traverses the array and anyone new will be put it in channel. It is **schedule**'s responsibility to handler the new registration and worker failure.
* When the second **schedule** is done, **run** calls **killWorkers** by send **Shutdown** RPC to all workers it recorded sequentially. Then it stop its RPC listener by a shutdown channel. Then **merge** is called to merge all reduce files and sort it. At last, sends 'done' to **Wait** channel to end blocking in the main routine.
* Master node offers **Register** RPC to called by workers. It appends *worker* member and waitup the waiting **forwardRegistration**
### Worker Framework
* The worker node calls **RunWorker** to run.
* **RunWorker** first call master's RPC **Register** to notice master.
* Then it opens worker RPC server in a similar way to master's manner, except that its listener goroutine is simply the main goroutine. I think it because worker node has nothing to do other than listening the request.
* Worker offers 2 RPC: **DoTask** and **Shutdown**. **ShutDown** can stop the listener goroutine by altering *nRPC* **DoTask** choose the **doMap** or **doReduce** by the parameter. It is the latters responsibility to read from file and save result to file and call user-defined **mapF** and **reduceF**.

## My Implementation
### Part I: Map/Reduce input and output
* **doMap** takes input file's name and its content as 'in kv' and call **mapF**. **mapF** returns multiple 'intermediate kvs'. **doMap** then creates *nReduce* empty files and puts all 'kvs' in them by hashing the key. I use encoder in *json* to marshal the kvs. So there will be *nMap* * *nReduce* intermediate files.
* **doReduce** reads its corresponding *nMap* intermediate files. To merge the same key's value, I call **sort.Slice** to kvs to sort it by the key. Then I traverse kvs, append values unless the key has changed. **reduceF** is called with every unique key and its values. **reduceF** return ultimate value and **doReduce** writes it in final file with the key by *json*'s Encoder. So *nReduce* final files will be created at last.

### Part II: Single-worker word count
* **mapF** takes every unique word as the intermediate key and takes the time it appeals in the file as the intermediate value.
* **reduceF** get the value add all the value of one key together.

### Part III: Distributing MapReduce tasks
* The responsibility of **schedule** is to call **DoTask** RPC to send *ntasks* tasks to several workers successfully and wait all tasks is finish before return. **DoTask** call should be paralleled.
* Because parallelism is needed, so every RPC **call** should in a new goroutine and 'waiting for finish' can be handled by channel.
* Since the worker never fails in this part, the task number is invariable. So the main work is to find worker.
* Worker comes by two means: registration and reuse after finishing the task. I use a *doneChan* in 'call goroutine' to notice **schedule** which worker has finished. So I wait for worker by two channel, and a *select* keyword is used to get either.
* To wait all tasks finished, I use a counter *doneN* which add everytime a *doneChan* is filled. And after all tasks is distributed, *doneChan* should still be listened until the *doneN* equals to ntasks.

```go
func schedule(...){
    ...

	doneN := 0
	doneChan := make(chan string)
	for taskIdx := 0; taskIdx < ntasks; taskIdx++ {
		var worker string
		select {
		case worker = <-registerChan:
		case worker = <-doneChan:
			doneN++
		}
		arg := DoTaskArgs{
			JobName:       jobName,
			File:          "",
			Phase:         phase,
			TaskNumber:    taskIdx,
			NumOtherPhase: n_other,
		}
		if phase == mapPhase {
			arg.File = mapFiles[taskIdx]
		}
		go func(ttt int) {
			ok := call(worker, "Worker.DoTask", arg, nil)
			if !ok {
				debug("Schedule: assign %v task %v to worker %v fail\n", arg.Phase, arg.TaskNumber, worker)
			}
			doneChan <- worker
		}(taskIdx)
	}
	for i := doneN; i < ntasks; i++ {
		<-doneChan
	}

    ...
}
```

### Part IV: Handling worker failures
* In this part worker can fail, so the task number may be more than *ntasks* now.
* So the *doneChan* should have more information. I use *DoTaskResult* to return if the task is done and if done the worker address is needed to reuse the worker, or the *taskIdx* is needed to redo the task.
* The done result is received in the process of finding the worker. But when task fails, the worker found cannot be used again and we get a new task. So I use a queue to store the todo tasks(go has no 'queue' so I use slice and a head idx instead) and fail task can be append to the end. I use a loop out side the *select*, so that I can get a usable worker address outside the loop.
* Then is the task. In most case, we can get it by simply poping the queue from the front. But when the queue is empty, it means all tasks is distributed but some tasks may fail later. So the newly found worker cannot be used now. I put it in a *idleWorkers* queue. The only way they can be used is when a previously-distributed task fails and should be redo. Then we should not wait worker in the *select* because it is slow and may even dead lock. Just pop the *idleWorkers* if it's not empty every time a task fails. And so the previous *select* will not change although I have another way to get worker.
* At last the only way to finish schedule is still increasing *doneN* to ntasks. Although not used in this lab, I also add a *doneMap* to handler the repeat success message for several reason.
```go
type DoTaskResult struct {
	Ok      bool
	TaskIdx int
	Worker  string
}

func schedule(...){
    ...

	doneChan := make(chan DoTaskResult)
	doneMap := make(map[int]struct{})
	doneN := 0
	taskQu := make([]int, ntasks)
	taskHead := 0
	for i := 0; i < ntasks; i++ {
		taskQu[i] = i
	}
	idleWorkers := make([]string, 0)
	idleHead := 0
DoneAll:
	for {
		var worker string
		var taskIdx int
		for {
			worker = ""
			for worker == "" {
				select {
				case worker = <-registerChan:
				case result := <-doneChan:
					if result.Ok {
						_, already := doneMap[result.TaskIdx]
						if !already {
							doneN++
							doneMap[result.TaskIdx] = struct{}{}
							if doneN == ntasks {
								break DoneAll
							}
						}
						worker = result.Worker
					} else {
						taskQu = append(taskQu, result.TaskIdx)
						if len(idleWorkers) != idleHead {
							worker = idleWorkers[idleHead]
							idleHead++
						}
					}
				}
			}
			if len(taskQu) != taskHead {
				taskIdx = taskQu[taskHead]
				taskHead++
				break
			}
			idleWorkers = append(idleWorkers, worker)
		}

		arg := DoTaskArgs{
			JobName:       jobName,
			File:          "",
			Phase:         phase,
			TaskNumber:    taskIdx,
			NumOtherPhase: n_other,
		}
		if phase == mapPhase {
			arg.File = mapFiles[taskIdx]
		}

		go func() {
			ok := call(worker, "Worker.DoTask", arg, nil)
			if !ok {
				fmt.Printf("Schedule: assign %v task %v to worker %v fail\n", arg.Phase, arg.TaskNumber, worker)
				doneChan <- DoTaskResult{Ok: false, TaskIdx: taskIdx, Worker: worker}
			} else {
				doneChan <- DoTaskResult{Ok: true, TaskIdx: taskIdx, Worker: worker}
			}
		}()
	}

    ...
}
```

### Part V: Inverted index generation (OPTIONAL)
* **mapF** takes every word appeals in the file as intermediate key and takes the filename as intermediate value.
* **reduceF** get the value by concatenating all the values of one key
